{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('swinface_project/')\n",
    "# packages\n",
    "\n",
    "from model import build_model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Face Alignment Phrase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Alignment base opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import face_utils\n",
    "import argparse\n",
    "import imutils\n",
    "import dlib\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('aligment/shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "def rotate(right_eye, left_eye, image):\n",
    "    midpoint = ((left_eye[0] + right_eye[0]) / 2.0, (left_eye[1] + right_eye[1]) / 2.0)\n",
    "    angle = np.arctan2(right_eye[1] - left_eye[1], right_eye[0] - left_eye[0]) * 180 / np.pi\n",
    "    midpoint = tuple(midpoint)\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(midpoint, angle, scale=1)\n",
    "    rotated_image = cv2.warpAffine(image, rotation_matrix, (image.shape[1], image.shape[0]))\n",
    "    return rotated_image,rotation_matrix\n",
    "def rotate_point(point,rotation_matrix):\n",
    "    # Convert coordinates to NumPy arrays for convenience\n",
    "    point = np.array(point)\n",
    "\n",
    "    # Add a row [0, 0, 1] to represent homogeneous coordinates\n",
    "    point_homogeneous = np.append(point, 1)\n",
    "\n",
    "    # Apply the rotation matrix\n",
    "    rotated_point_homogeneous = np.dot(rotation_matrix, point_homogeneous)\n",
    "\n",
    "    # Convert back to coordinates without homogeneous coordinates\n",
    "    rotated_point = rotated_point_homogeneous[:2]\n",
    "\n",
    "    return tuple(map(int, rotated_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignment_opencv(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    rects = detector(gray, 1)\n",
    "    # if detector more than one face\n",
    "    if len(rects) !=1 :\n",
    "        return None\n",
    "    shape = predictor(gray, rects[0])\n",
    "    shape = face_utils.shape_to_np(shape)\n",
    "    right_eye = shape[25]\n",
    "    left_eye = shape[20]\n",
    "    rotate_img= rotate(right_eye,left_eye,image)\n",
    "    right_eye = shape[25]\n",
    "    left_eye = shape[20]\n",
    "    rotate_img,rotate_matrix = rotate(right_eye,left_eye,image)\n",
    "    x_max = shape[0][0]\n",
    "    y_max = shape[0][1]\n",
    "    x_min = shape[0][0]\n",
    "    y_min = shape[0][1]\n",
    "    for (x,y) in shape:\n",
    "        (x,y) = rotate_point((x,y),rotate_matrix)\n",
    "        if x_max < x:\n",
    "            x_max = x\n",
    "        elif x_min > x:\n",
    "            x_min = x\n",
    "        elif y_max < y:\n",
    "            y_max = y\n",
    "        elif y_min > y:\n",
    "            y_min = y\n",
    "    crop_img = rotate_img[y_min-60:y_max+60,x_min-40:x_max+40]\n",
    "    return crop_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_img = 'img/008.jpg'\n",
    "img = cv2.imread(path_img)\n",
    "alignment_img = alignment_opencv(img)\n",
    "cv2.imwrite(\"img/alignment_img_008.jpg\",alignment_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Alignment base Star (HairFastGan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Face Analysis Phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Init parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinFaceCfg:\n",
    "    network = \"swin_t\"\n",
    "    fam_kernel_size=3\n",
    "    fam_in_chans=2112\n",
    "    fam_conv_shared=False\n",
    "    fam_conv_mode=\"split\"\n",
    "    fam_channel_attention=\"CBAM\"\n",
    "    fam_spatial_attention=None\n",
    "    fam_pooling=\"max\"\n",
    "    fam_la_num_list=[2 for j in range(11)]\n",
    "    fam_feature=\"all\"\n",
    "    fam = \"3x3_2112_F_s_C_N_max\"\n",
    "    embedding_size = 512\n",
    "    \n",
    "cfg = SwinFaceCfg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Create/load swin-face model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_swinface_model\n",
    "def create_swinface_model(cfg, weight_path):\n",
    "    model = build_model(cfg)\n",
    "    dict_checkpoint = torch.load(weight_path)\n",
    "    model.backbone.load_state_dict(dict_checkpoint[\"state_dict_backbone\"])\n",
    "    model.fam.load_state_dict(dict_checkpoint[\"state_dict_fam\"])\n",
    "    model.tss.load_state_dict(dict_checkpoint[\"state_dict_tss\"])\n",
    "    model.om.load_state_dict(dict_checkpoint[\"state_dict_om\"])\n",
    "\n",
    "    model.eval()\n",
    "    return model \n",
    "\n",
    "weight_path = \"swinface_project/checkpoint_step_79999_gpu_0.pt\"\n",
    "swin_face_model = create_swinface_model(cfg, weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Inference on one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def inference(model, img):\n",
    "    if img is None:\n",
    "        img = np.random.randint(0, 255, size=(112, 112, 3), dtype=np.uint8)\n",
    "    else:\n",
    "\n",
    "        img = cv2.resize(img, (112, 112))\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    img = torch.from_numpy(img).unsqueeze(0).float()\n",
    "    img.div_(255).sub_(0.5).div_(0.5)\n",
    "    \n",
    "    output = model(img)#.numpy()\n",
    "\n",
    "    return output[\"Recognition\"]\n",
    "\n",
    "image = cv2.imread(\"img/001.jpg\")\n",
    "facial_embedding = inference(swin_face_model, image)\n",
    "facial_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Inference on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 512])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# facial embedding for all images in directory (jpg and png)\n",
    "def inference_in_dir_path(model, dir_path):\n",
    "    files = os.listdir(dir_path)\n",
    "    files = [f for f in files if f.endswith(\".jpg\") or f.endswith(\".png\")]\n",
    "\n",
    "    embeddings = []\n",
    "    for f in files:\n",
    "        img = cv2.imread(os.path.join(dir_path, f))\n",
    "        facial_embedding = inference(model, img)\n",
    "        embeddings.append(facial_embedding)\n",
    "\n",
    "    return files, torch.stack(embeddings, dim=0)\n",
    "\n",
    "dir_path = \"img\"\n",
    "list_of_image, embeddings = inference_in_dir_path(swin_face_model, dir_path)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Caculate the cosine similarity between two embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.2042]),\n",
       " tensor([1.0000]),\n",
       " tensor([0.1023]),\n",
       " tensor([0.1435]),\n",
       " tensor([0.3496])]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# caculate similarity between 2 embeddings (torch.tensor)\n",
    "def similarity(embedding1, embedding2):\n",
    "    return torch.nn.functional.cosine_similarity(embedding1, embedding2)\n",
    "\n",
    "# caculate similarity between embeddings and a specific embedding\n",
    "def similarity_with_specific_embedding(embeddings, specific_embedding):\n",
    "    list_of_scores = []\n",
    "    for i in range(embeddings.shape[0]):\n",
    "        sim = similarity(embeddings[i], specific_embedding)\n",
    "        list_of_scores.append(sim)\n",
    "    return list_of_scores\n",
    "    \n",
    "sim_scores = similarity_with_specific_embedding(embeddings, embeddings[1])\n",
    "sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "congnt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
